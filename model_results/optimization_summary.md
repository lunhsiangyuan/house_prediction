# 房價預測模型優化總結

## 優化方案概述

我們實施了三種主要的優化方案，以提高房價預測模型的性能：

1. **高級特徵工程**：
   - 多項式特徵：為重要特徵創建二次多項式特徵，捕捉非線性關係
   - 主成分分析 (PCA)：創建15個主成分特徵，累積解釋方差達73.58%
   - 聚類特徵：使用K-means聚類創建聚類標籤和距離特徵
   - 特徵總數從100個增加到148個

2. **貝葉斯優化XGBoost**：
   - 使用貝葉斯優化方法調整XGBoost超參數
   - 相比傳統的網格搜索和隨機搜索，更有效地探索超參數空間
   - 最佳參數：
     - learning_rate: 0.01
     - n_estimators: 1000
     - max_depth: 4
     - min_child_weight: 0.5
     - subsample: 0.5
     - colsample_bytree: 1.0
     - gamma: 0.001
     - reg_alpha: 0.001
     - reg_lambda: 10.0
   - 交叉驗證RMSE: 0.125640 (對數空間)

3. **堆疊集成模型**：
   - 結合7個基礎模型：Ridge、Lasso、ElasticNet、隨機森林、梯度提升樹、XGBoost、LightGBM
   - 使用Ridge回歸作為元模型，學習如何最佳組合基礎模型的預測
   - 交叉驗證RMSE: 0.124580 (對數空間)

## 性能比較

| 模型 | 交叉驗證 RMSE (對數空間) | 訓練時間 | 參數優化 |
|------|------------------------|---------|---------|
| XGBoost (優化) | 0.125871 | ~3.5分鐘 | 網格搜索 + 隨機搜索 |
| LightGBM | 0.134672 | ~0.1分鐘 | 預設參數微調 |
| 隨機森林 | 0.142635 | ~1.2分鐘 | 基本參數設定 |
| SVR | 0.147298 | ~5分鐘 | 僅使用最重要特徵 |
| 集成模型 (加權平均) | 0.135640 | ~0.2分鐘 | XGBoost(0.40), LightGBM(0.40), RF(0.20) |
| 貝葉斯優化XGBoost | 0.125640 | ~0.1分鐘 | 貝葉斯優化 |
| 堆疊集成 | 0.124580 | ~0.2分鐘 | 堆疊集成 (7個基礎模型) |

## 結論與建議

1. **最佳模型**：堆疊集成模型表現最佳，RMSE為0.124580，略優於貝葉斯優化XGBoost和原始優化XGBoost。

2. **效率考量**：
   - LightGBM在速度和性能之間取得了良好的平衡，訓練時間短且性能不錯
   - 貝葉斯優化XGBoost在保持高性能的同時，大幅減少了訓練時間

3. **特徵工程的影響**：
   - 高級特徵工程增加了特徵數量，但並未顯著提高模型性能
   - 這表明原始特徵工程已經捕捉了大部分有用信息

4. **集成方法的效果**：
   - 堆疊集成比簡單的加權平均集成效果更好
   - 這表明不同模型捕捉了數據中的不同模式，通過元模型可以更好地組合這些信息

5. **後續改進方向**：
   - 實施SHAP值分析和偏依存圖分析，進一步理解模型預測機制
   - 進行模型殘差分析，找出模型預測效果較差的情況
   - 嘗試神經網路模型，特別是針對特徵間的複雜交互
   - 開發模型API和網頁界面，實現實時預測服務

總體而言，我們的優化工作成功地提高了模型性能，特別是通過堆疊集成方法。這些改進使我們的房價預測模型更加準確和穩健，為後續的模型部署和應用奠定了良好的基礎。 